

\documentclass[12pt]{article} % use larger type; default would be 10pt


\usepackage{graphicx} 
\usepackage{framed}



\title{Hibernia College}
\author{Kevin O'Brien}

\begin{document}
\maketitle
%---------------------------------------------------------------------------------------------------%
\begin{verbatim}
1) MA4704 Midterms - finalize
2) MA4128
-----------------------------------------------------
Missing Data
Pairwise Deletion
Listwise Deletion
Multiple Imputation
Replace with the mean value
Bayesian Approach 
-----------------------------------------------------
Missing at Random (MCAR)
Missing Not At Random (MNAR)
Missing Completely at Random (MCAR)
Censored Data
Left Censored Data (Not Part of Course)
Right Censored Data (Not Part of Course)
-----------------------------------------------------
Classification
Misclassification
Training and Validation
False Positive and False Negative
Confusion Matrix
Specifity and Sensitivity
Accuracy
Recall
Precision
True Error Rate
Apparent Error Rate
-----------------------------------------------------

http://www.jstor.org/discover/10.2307/1266219?uid=2&uid=4&sid=21102180625907

OC or ROC diagram for 2-class problems. (OC = operating characteristic; ROC= receiver operating characteristic.)

Detection, false alarm.

\end{verbatim}

\newpage








%-----------------------------------------------------------------------------------%




\subsection{What Is Discriminant Analysis?}

Discriminant analysis is a \textbf{classification} method. 

% It assumes that different classes generate data based on different Gaussian distributions.

\begin{itemize}
\item To train (create) a classifier, the fitting function estimates the parameters of a Gaussian distribution for each class.
\item To predict the classes of new data, the trained classifier finds the class with the smallest misclassification cost.
\end{itemize}
%-----------------------------------------------------------------------------------%
\subsection{Recall and Precision}

In a classification task, the precision for a class is the number of true positives (i.e. the number of items correctly labeled as belonging to the positive class) divided by the total number of elements labeled as belonging to the positive class (i.e. the sum of true positives and false positives, which are items incorrectly labeled as belonging to the class). Recall in this context is defined as the number of true positives divided by the total number of elements that actually belong to the positive class (i.e. the sum of true positives and false negatives, which are items which were not labeled as belonging to the positive class but should have been).
%-----------------------------------------------------------------------------------%

\subsection*{Cost}

There are two costs associated with discriminant analysis classification: the true misclassification cost per class, and the expected misclassification cost per observation.

True Misclassification Cost per Class.  Cost(i,j) is the cost of classifying an observation into class j if its true class is i. 

By default, Cost(i,j)=1 if i~=j, and Cost(i,j)=0 if i=j. 

In other words, the cost is 0 for correct classification, and 1 for incorrect classification.

B is a square matrix of size K-by-K when there are K classes. 

\subsection{Expected Misclassification Cost per Observation}.  

Suppose you have Nobs observations that you want to classify with a trained discriminant analysis classifier obj. 

Suppose you have K classes. You place the observations into a matrix Xnew with one observation per row. 

%-------------------------------------------------------------------------------------%

% https://onlinecourses.science.psu.edu/stat505/node/78

% http://rer.sagepub.com/content/45/4/543.full.pdf

%---------------------------------------------------------------------------------------%

\subsection*{Cross Validation}

As in all statistical procedures it is helpful to use diagnostic procedures to asses the efficacy of the discriminant analysis. 

We use cross-validation to assess the classification probability. 

Typically you are going to have some prior rule as to what is an acceptable misclassification rate. 

Those rules might involve things like, ``what is the cost of misclassification?" 

This could come up in a medical study where you might be able to diagnose cancer. 

There are really two alternative costs. The cost of misclassifying someone as having cancer when they don't. 

This could cause a certain amount of emotional grief.

%-------------------------------------------------------------------------------------%

\subsection*{Sensitivity and specificity}

Sensitivity and specificity are statistical measures of the performance of a binary classification test, also known in statistics as classification function. Sensitivity (also called the true positive rate, or the recall rate in some fields) measures the proportion of actual positives which are correctly identified as such (e.g. the percentage of sick people who are correctly identified as having the condition). Specificity measures the proportion of negatives which are correctly identified as such (e.g. the percentage of healthy people who are correctly identified as not having the condition, sometimes called the true negative rate). 

\[ \mbox{sensitivity} = \frac{ \mbox{number of true positives} } {\mbox{number of true positives} + \mbox{number of false negatives}} \]

%-------------------------------------------------------------------------------------%

\subsection*{Confusion Matrix}

In predictive analytics, a table of confusion (sometimes also called a confusion matrix), is a table with two rows and two columns that reports the number of false positives, false negatives, true positives, and true negatives. This allows more detailed analysis than mere proportion of correct guesses (accuracy). Accuracy is not a reliable metric for the real performance of a classifier, because it will yield misleading results if the data set is unbalanced (that is, when the number of samples in different classes vary greatly). For example, if there were 95 cats and only 5 dogs in the data set, the classifier could easily be biased into classifying all the samples as cats. The overall accuracy would be 95\%, but  in practice the classifier would have a 100\% recognition rate for the cat class but a 0\% recognition rate for the dog class.

%-------------------------------------------------------------------------------------%

There is also the alternative cost of misclassifying someone as not having cancer when in fact they do have it! 

The cost here is obviously greater if early diagnosis improves cure rates.

%-----------------------------------------------------------------------------------%

We can evaluate error rates by means of a training sample (to construct thevdiscrimination surface) and a test sample.

An optimistic error rate is obtained by reclassifying the design set: this is known as the \textbf{apparent error rate}.

If an independent test sample is used for classifying, we arrive at the true error rate.

The leaving one out method attempts to use as much of the data as possible: for every subset of $n-1$ objects from the given objects, a classifier is designed, 
and the object omitted is assigned. This leads to the overhead of discriminant analyses, and to tests from which an error rate can be derived.

Another approach to appraising the results of a discriminant analysis is to determine a confusion matrix which is a contingency table (a table of frequencies of coâ€“occurrence) crossing the known groups with the obtained groups.






%http://www.upa.pdx.edu/IOA/newsom/semclass/ho_missing.pdf

%------------------------------------------------------%


%http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Missing_Data/Missing.html

% http://www.uvm.edu/~dhowell/StatPages/More_Stuff/Missing_Data/Missing.html
% http://www.stat.columbia.edu/~gelman/arm/missing.pdf
%---------------------------------------------------------------------------------------------------%
\begin{itemize}
\item \texttt{roots()} roots of a polynomial
\item \texttt{poly()} characteristic equation
\item \texttt{primes()} generate a sequence of prime numbers
\item \texttt{isprime()} check for prime numbers
\item \texttt{any()} check if any value fulfils a logical condition
\item \texttt{all()} check if all value fulfils a logical condition
\end{itemize}

 \newpage

\textbf{Grubbs Test}

p: value



The null hypothesis is that both sampmes are drawn from the same population of values.

The test is a non parametric test. Non Parametric tests are a family of inference tests that do not require the assumption of normality.
\subsection{Grubbs's Test for Outliers}
Grubbs Testnis is used to determine whether or not a particular value in the data set is an \textbf{\textit{outlier}}. There is no standard definition of what defines an outlier. The definition of an outlier used for this procedure is a value that is numerically distance from the rest of the values of the data set. (We will refer to such an outlier as a \textbf{\textit{Grubbs outlier}}.


Another type of outlier is a point identified as such by a boxplot. We will refer to this type of outlier as a \textbf{\textit{boxplot outlier}}.  An outlier may be either one or the other type, but not necessarily both.

Some outliers may be due to incorrectly recorded data. Other outliers are correctly recorded, but very unusual, values.

null $H_0$ : There are no outliers present in the data set.

alt $H_1$ : There is an outlier present in the data set.

\subsubsection{Implementation using \texttt{R}}
To implement the Grubbs test, we require the package \textbf{\textit{outliers}}.

%--------------------------------------------------------------------------------%
\subsection{Kolmorgorov Smirnov Test}
Two sample KS test

The Kolmorogorov Smirnov Test (aka The KS test) is used to determine whether or not two data sets are from the same distribution.
The test is implemented in \texttt{R}n using the command \texttt{ks.test()}. No packages are required to run this procedure.




\begin{framed}
\begin{verbatim}
x=c(3,4,5,1,6)
library(outliers)
grubbs.test(X,two.tailed=T)
\end{verbatim}
\end{framed}




\section{graph theory }
Given the following definitions for simple, connected graphs:
\begin{itemize}
\item $K_n$ is a graph on $n$ vertices where each pair of vertices is connected by an edge;
\item $C_n$ is the graph with vertices $v_1, v_2, v_3, \dots, v_n$ and edges $\{v_1,v_2\}, \{v_2,v_3\}, \dots\{v_n, v_1\}$;
\item $W_n$ is the graph obtained from $C_n$ by adding an extra vertex,$v_{n+1}$, and edges
from this to each of the original vertices in $C_n$.
\end{itemize}
(a) Draw $K_4$, $C_4$, and $W_4$. 
%----------------------------------------------------------------%
